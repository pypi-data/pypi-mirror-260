"""Main module."""

from logging import Logger
from pathlib import Path
from typing import Any, List, Optional

from universal_data_permissions_scanner import (
    AwsAssumeRoleInput,
    AWSAuthzAnalyzer,
    BigQueryAuthzAnalyzer,
    MongoDBAtlasAuthzAnalyzer,
    MongoDBAuthzAnalyzer,
    PostgresAuthzAnalyzer,
    RedshiftAuthzAnalyzer,
    SnowflakeAuthzAnalyzer,
)
from universal_data_permissions_scanner.writers import OutputFormat, get_writer

from universal_data_permissions_scanner.datastores.databricks.analyzer import DatabricksAuthzAnalyzer
from universal_data_permissions_scanner.datastores.databricks import Authentication

from universal_data_permissions_scanner.writers.base_writers import BaseWriter


def run_snowflake(
    logger: Logger,
    username: str,
    password: Optional[str],
    account: str,
    warehouse: Optional[str],
    output_format: OutputFormat,
    output_path: Path,
    rsa_key: Optional[str],
    rsa_pass: Optional[str],
    **kwargs: Any,
) -> None:
    """Run snowflake analyzer.

    Args:
        logger (Logger): Logger
        username (str): Username
        password (str): Password
        account (str): Snowflake account to analyzer
        warehouse (str): Warehouse
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
        kwargs:
            host (str): Snowflake host
    """
    custom_writer = kwargs.pop("writer")
    snowflake_analyzer = SnowflakeAuthzAnalyzer.connect(
        username=username,
        password=password,
        warehouse=warehouse,
        account=account,
        output_path=output_path,
        output_format=output_format,
        logger=logger,
        rsa_key=rsa_key,
        rsa_pass=rsa_pass,
        custom_writer=custom_writer,
        kwargs=kwargs,
    )
    snowflake_analyzer.run()


# AWS S3 runner
def run_aws_s3(
    logger: Logger,
    output_format: OutputFormat,
    filename: str,
    target_account: AwsAssumeRoleInput,
    additional_accounts: Optional[List[AwsAssumeRoleInput]] = None,
    **kwargs: Any,
) -> None:
    writer: BaseWriter = kwargs.get("writer", get_writer(filename, output_format))
    analyzer = AWSAuthzAnalyzer.connect(
        target_account=target_account,
        additional_accounts=additional_accounts,
        logger=logger,
        output_path=filename,
        output_format=output_format,
        custom_writer=writer,
    )
    analyzer.run_s3()
    writer.close()


def run_bigquery(logger: Logger, project_id: str, output_format: OutputFormat, output_path: str, **kwargs: Any) -> None:
    """Run BigQuery analyzer.

    Args:
        logger (Logger): Logger
        project_id (str): BigQuery project ID to scan
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
    """
    writer = kwargs.get("writer")
    analyzer = BigQueryAuthzAnalyzer.connect(
        logger=logger, project_id=project_id, output_path=output_path, output_format=output_format, custom_writer=writer
    )
    analyzer.run()


def run_postgres(
    logger: Logger,
    username: str,
    password: str,
    host: str,
    dbname: str,
    output_format: OutputFormat,
    output_path: Path,
    port: int,
    **kwargs: Any,
) -> None:
    """Run Postgres analyzer.

    Args:
        logger (Logger): Logger
        username (str): Postgres username
        password (str): Postgres password
        host (str): FQDN or IP of the postgres DB
        dbname (str): Postgres database name, for example postgres
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
        port (int): Postgres port
    """
    writer = kwargs.get("writer")
    analyzer = PostgresAuthzAnalyzer.connect(
        username=username,
        password=password,
        host=host,
        dbname=dbname,
        output_path=output_path,
        output_format=output_format,
        logger=logger,
        port=port,
        custom_writer=writer,
    )
    analyzer.run()


def run_redshift(
    logger: Logger,
    username: str,
    password: str,
    host: str,
    dbname: str,
    output_format: OutputFormat,
    output_path: Path,
    port: int,
    **kwargs: Any,
) -> None:
    """Run Redshift analyzer.

    Args:
        logger (Logger): Logger
        username (str): Redshift username
        password (str): Redshift password
        host (str): FQDN or IP of the redshift DB
        dbname (str): Redshift database name, for example dev
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
        port (int): Redshift port
    """
    writer = kwargs.get("writer")
    analyzer = RedshiftAuthzAnalyzer.connect(
        username=username,
        password=password,
        host=host,
        dbname=dbname,
        output_path=output_path,
        output_format=output_format,
        logger=logger,
        port=port,
        custom_writer=writer,
    )
    analyzer.run()


def run_mongodb(
    logger: Logger,
    username: str,
    password: str,
    host: str,
    output_format: OutputFormat,
    output_path: Path,
    port: int,
    ssl: bool,
    **kwargs: Any,
) -> None:
    """Run MongoDB analyzer.

    Args:
        logger (Logger): Logger
        username (str): username
        password (str): password
        host (str): FQDN or IP of the MongoDB DB
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
        port (int): port
        ssl(bool): use ssl
    """
    writer = kwargs.get("writer")
    analyzer = MongoDBAuthzAnalyzer.connect(
        username=username,
        password=password,
        host=host,
        output_path=output_path,
        output_format=output_format,
        logger=logger,
        port=port,
        ssl=ssl,
        custom_writer=writer,
    )
    analyzer.run()


def run_mongodb_atlas(
    logger: Logger,
    public_key: str,
    private_key: str,
    username: str,
    password: str,
    project_name: str,
    cluster_name: str,
    output_format: OutputFormat,
    output_path: Path,
    **kwargs: Any,
) -> None:
    """Run MongoDB Atlas analyzer.

    Args:
        logger (Logger): Logger
        public_key (str): MongoDB Atlas public key, generated through the organization access manager
        private_key (str): MongoDB Atlas public key, generated through the organization access manager
        username (str): MongoDB cluster username
        password (str): MongoDB cluster password
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
    """
    writer = kwargs.get("writer", None)
    analyzer = MongoDBAtlasAuthzAnalyzer.connect(
        public_key=public_key,
        private_key=private_key,
        db_user=username,
        db_password=password,
        project_name=project_name,
        cluster_name=cluster_name,
        output_path=output_path,
        output_format=output_format,
        logger=logger,
        custom_writer=writer,
    )
    analyzer.run()


def run_databricks(
    logger: Logger,
    host: str,
    authentication: Authentication,
    account_id: str,
    output_format: OutputFormat,
    output_path: Path,
    **kwargs: Any,
) -> None:
    """Run Databricks analyzer.

    Args:
        logger (Logger): Logger
        host (str): workspace host, e.g. https://<workspace>.cloud.databricks.com
        key (str): Databricks API key
        output_format (OutputFormat): Output format, CSV or JSON
        output_path (str): Where to write the output
        authentication: Authentication method
    """
    writer = kwargs.get("writer", None)
    analyzer = DatabricksAuthzAnalyzer.connect(
        host=host,
        authentication=authentication,
        account_id=account_id,
        output_path=output_path,
        output_format=output_format,
        logger=logger,
        custom_writer=writer,
    )
    analyzer.run()
