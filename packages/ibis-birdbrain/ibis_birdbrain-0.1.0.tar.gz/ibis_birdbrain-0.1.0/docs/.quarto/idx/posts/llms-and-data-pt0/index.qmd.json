{"title":"An introduction to Marvin and Ibis","markdown":{"yaml":{"title":"An introduction to Marvin and Ibis","author":"Cody Peterson","date":"2023-10-12","execute":{"warning":false},"categories":["LLMs and data"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nIn this \"LLMs and data\" series, we'll explore how to apply large-language models\n(LLMs) to data analytics. We'll walk through the steps to build Ibis Birdbrain.\n\nThroughout the series, we'll be using\n[Marvin](https://www.askmarvin.ai/welcome/overview/) and\n[Ibis](https://ibis-project.org). A brief introduction to each is provided\nbelow.\n\n## Marvin\n\n[Marvin](https://www.askmarvin.ai/welcome/overview/) is an AI engineering\nframework that makes it easy to build up to an interactive conversational\napplication.\n\nMarvin makes calls to an AI platform. You typically use an API key set as an\nenvironment variable -- in this case, we'll load a `.env` file that contians\nsecrets for the AI platform that Marvin will use. We also set the large language\nmodel model.\n\n```{python}\nimport marvin  # <1>\n\nfrom rich import print  # <1>\nfrom time import sleep  # <1>\nfrom dotenv import load_dotenv  # <1>\n\nload_dotenv()  # <2>\n\n# increase accuracy\nmarvin.settings.llm_model = \"openai/gpt-4\"  # <3>\n# decrease cost\n# marvin.settings.llm_model = \"openai/gpt-3.5-turbo\"  # <3>\n\ntest_str = \"working with data and LLMs on 18+ data platforms is easy!\"  # <4>\ntest_str\n```\n\n1. Import the libraries we need.\n2. Load the environment variable to setup Marvin to call our OpenAI account.\n3. Configure the LLM model to use.\n4. Some text to test on\n\n### Functions\n\nAI functions are one of the building blocks in Marvin and allow yout to specify\na typed python function with no code -- only a docstring -- to achieve a wide\nvariety of tasks.\n\nWe'll demonstrate this with an AI function that trnaslates text:\n\n```{python}\n@marvin.ai_fn\ndef translate(text: str, from_: str = \"English\", to: str = \"Spanish\") -> str:\n    \"\"\"translates the text\"\"\"\n\ntranslate(test_str)\n```\n\n```{python}\n# | code-fold: true\nsleep(1) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\ntranslate(translate(test_str), from_=\"Spanish\", to=\"English\")\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n### Models\n\nAI models are another building block for generating python classes from input\ntext. It's a great way to build structured data from unstructured data that can\nbe customized for your needs. \n\nWe'll demosntrate this with an AI model that translates text:\n\n```{python}\nfrom pydantic import BaseModel, Field\n\n# decrease cost\nmarvin.settings.llm_model = \"openai/gpt-3.5-turbo\"\n\n@marvin.ai_model\nclass ExtractParts(BaseModel):\n    \"\"\"Extracts parts of a sentence\"\"\"\n    subject: str = Field(..., description=\"The subject of the sentence.\")\n    objects: list[str] = Field(..., description=\"The objects of the sentence.\")\n    predicate: str = Field(..., description=\"The predicate of the sentence.\")\n    modifiers: list[str] = Field(..., description=\"The modifiers of the sentence.\")\n\nExtractParts(test_str)\n```\n\n```{python}\n# | code-fold: true\nsleep(1) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n### Classifiers\n\nAI classifiers are another building block for generating python classes from\ninput text. It's the most efficient (time and cost) method for applying LLMs as\nit only results in a single output token, selecting an output in a specified\nEnum.\n\nWe'll demonstrate this by classifying the language of some text:\n\n```{python}\nfrom enum import Enum\n\n# increase accuracy\nmarvin.settings.llm_model = \"openai/gpt-4\"\n\n@marvin.ai_classifier\nclass IdentifyLanguage(Enum):\n    \"\"\"Identifies the language of the text\"\"\"\n\n    english = \"English\"\n    spanish = \"Spanish\"\n\n\nIdentifyLanguage(test_str).value\n```\n\n```{python}\n# | code-fold: true\nsleep(1) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nIdentifyLanguage(translate(test_str)).value\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n## Ibis\n\n[Ibis](https://ibis-project.org) is the portable Python dataframe library that\nenables Ibis Birdbrain to work on many data platforms at native scale.\n\nIbis makes calls to a data platform, providing an API but pushing the compute to\n(local or remote) query engines and storage. DuckDB is the default and we'll\ntypically use it for demo puroses. You can work with an in-memory instance, but\nwe'll often create a database file from example data:\n\n```{python}\nimport ibis  # <1>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <2>\nt = ibis.examples.penguins.fetch()  # <2>\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)  # <2>\n```\n\n1. Import the libraries we need.\n2. Setup the demo datain an Ibis backend.\n\nYou will typically connect to an existing data platform via your corresponding\nIbis backend and have access to a number of tables:\n\n```{python}\nimport ibis  # <1>\n\nibis.options.interactive = True  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = con.table(\"penguins\")  # <3>\n```\n\n1. Import Ibis.\n2. Configure Ibis (interactive).\n3. Connect to the data and load a table into a variable.\n\n### Backend\n\nA backend provides the connection and basic management of the data platform.\nAbove, we created the `con` variable that is an instance of a DuckDB backend:\n\n```{python}\ncon\n```\n\nIt usually contains some tables:\n\n```{python}\ncon.list_tables()\n```\n\nWe can access some internals of Ibis to see what backends are available:\n\n::: {.callout-tip}\nDon't rely on accessing internals of Ibis in production.\n:::\n\n```{python}\nbackends = [entrypoint.name for entrypoint in ibis.util.backend_entry_points()]\nbackends\n```\n\n### Table\n\nYou typically work with a table, conventionally named `t` for demo or\nexploratory purposes:\n\n```{python}\nt\n```\n\nWhen working with many tables, you should name them descriptively.\n\n### Schema\n\nA table has a schema that Ibis maps to the data platform's data types:\n\n```{python}\nt.schema()\n```\n\n## LLMs and data: Marvin and Ibis\n\nYou can use Marvin and Ibis together to easily apply LLMs to data.\n\n```{python}\nfrom ibis.expr.schema import Schema\nfrom ibis.expr.types.relations import Table\n\n@marvin.ai_fn\ndef sql_select(\n    text: str, table_name: str = t.get_name(), schema: Schema = t.schema()\n) -> str:\n    \"\"\"writes the SQL SELECT statement to query the table according to the text\"\"\"\n\n\nquery = \"the unique combination of species and islands\"\nsql = sql_select(query).strip(\";\")\nsql\n```\n\n```{python}\nt.sql(sql)\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt.sql(sql_select(query + \" and include their counts in from highest to lowest\").strip(\";\"))\n```\n\n## Next steps\n\nYou can get involved with [Ibis\nBirdbrain](https://github.com/ibis-project/ibis-birdbrain), our open-source data\n& AI project for building next-generation natural language interfaces to data.\n\n[Read the next post in this series](../llms-and-data-pt1).\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nIn this \"LLMs and data\" series, we'll explore how to apply large-language models\n(LLMs) to data analytics. We'll walk through the steps to build Ibis Birdbrain.\n\nThroughout the series, we'll be using\n[Marvin](https://www.askmarvin.ai/welcome/overview/) and\n[Ibis](https://ibis-project.org). A brief introduction to each is provided\nbelow.\n\n## Marvin\n\n[Marvin](https://www.askmarvin.ai/welcome/overview/) is an AI engineering\nframework that makes it easy to build up to an interactive conversational\napplication.\n\nMarvin makes calls to an AI platform. You typically use an API key set as an\nenvironment variable -- in this case, we'll load a `.env` file that contians\nsecrets for the AI platform that Marvin will use. We also set the large language\nmodel model.\n\n```{python}\nimport marvin  # <1>\n\nfrom rich import print  # <1>\nfrom time import sleep  # <1>\nfrom dotenv import load_dotenv  # <1>\n\nload_dotenv()  # <2>\n\n# increase accuracy\nmarvin.settings.llm_model = \"openai/gpt-4\"  # <3>\n# decrease cost\n# marvin.settings.llm_model = \"openai/gpt-3.5-turbo\"  # <3>\n\ntest_str = \"working with data and LLMs on 18+ data platforms is easy!\"  # <4>\ntest_str\n```\n\n1. Import the libraries we need.\n2. Load the environment variable to setup Marvin to call our OpenAI account.\n3. Configure the LLM model to use.\n4. Some text to test on\n\n### Functions\n\nAI functions are one of the building blocks in Marvin and allow yout to specify\na typed python function with no code -- only a docstring -- to achieve a wide\nvariety of tasks.\n\nWe'll demonstrate this with an AI function that trnaslates text:\n\n```{python}\n@marvin.ai_fn\ndef translate(text: str, from_: str = \"English\", to: str = \"Spanish\") -> str:\n    \"\"\"translates the text\"\"\"\n\ntranslate(test_str)\n```\n\n```{python}\n# | code-fold: true\nsleep(1) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\ntranslate(translate(test_str), from_=\"Spanish\", to=\"English\")\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n### Models\n\nAI models are another building block for generating python classes from input\ntext. It's a great way to build structured data from unstructured data that can\nbe customized for your needs. \n\nWe'll demosntrate this with an AI model that translates text:\n\n```{python}\nfrom pydantic import BaseModel, Field\n\n# decrease cost\nmarvin.settings.llm_model = \"openai/gpt-3.5-turbo\"\n\n@marvin.ai_model\nclass ExtractParts(BaseModel):\n    \"\"\"Extracts parts of a sentence\"\"\"\n    subject: str = Field(..., description=\"The subject of the sentence.\")\n    objects: list[str] = Field(..., description=\"The objects of the sentence.\")\n    predicate: str = Field(..., description=\"The predicate of the sentence.\")\n    modifiers: list[str] = Field(..., description=\"The modifiers of the sentence.\")\n\nExtractParts(test_str)\n```\n\n```{python}\n# | code-fold: true\nsleep(1) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n### Classifiers\n\nAI classifiers are another building block for generating python classes from\ninput text. It's the most efficient (time and cost) method for applying LLMs as\nit only results in a single output token, selecting an output in a specified\nEnum.\n\nWe'll demonstrate this by classifying the language of some text:\n\n```{python}\nfrom enum import Enum\n\n# increase accuracy\nmarvin.settings.llm_model = \"openai/gpt-4\"\n\n@marvin.ai_classifier\nclass IdentifyLanguage(Enum):\n    \"\"\"Identifies the language of the text\"\"\"\n\n    english = \"English\"\n    spanish = \"Spanish\"\n\n\nIdentifyLanguage(test_str).value\n```\n\n```{python}\n# | code-fold: true\nsleep(1) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nIdentifyLanguage(translate(test_str)).value\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n## Ibis\n\n[Ibis](https://ibis-project.org) is the portable Python dataframe library that\nenables Ibis Birdbrain to work on many data platforms at native scale.\n\nIbis makes calls to a data platform, providing an API but pushing the compute to\n(local or remote) query engines and storage. DuckDB is the default and we'll\ntypically use it for demo puroses. You can work with an in-memory instance, but\nwe'll often create a database file from example data:\n\n```{python}\nimport ibis  # <1>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <2>\nt = ibis.examples.penguins.fetch()  # <2>\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)  # <2>\n```\n\n1. Import the libraries we need.\n2. Setup the demo datain an Ibis backend.\n\nYou will typically connect to an existing data platform via your corresponding\nIbis backend and have access to a number of tables:\n\n```{python}\nimport ibis  # <1>\n\nibis.options.interactive = True  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = con.table(\"penguins\")  # <3>\n```\n\n1. Import Ibis.\n2. Configure Ibis (interactive).\n3. Connect to the data and load a table into a variable.\n\n### Backend\n\nA backend provides the connection and basic management of the data platform.\nAbove, we created the `con` variable that is an instance of a DuckDB backend:\n\n```{python}\ncon\n```\n\nIt usually contains some tables:\n\n```{python}\ncon.list_tables()\n```\n\nWe can access some internals of Ibis to see what backends are available:\n\n::: {.callout-tip}\nDon't rely on accessing internals of Ibis in production.\n:::\n\n```{python}\nbackends = [entrypoint.name for entrypoint in ibis.util.backend_entry_points()]\nbackends\n```\n\n### Table\n\nYou typically work with a table, conventionally named `t` for demo or\nexploratory purposes:\n\n```{python}\nt\n```\n\nWhen working with many tables, you should name them descriptively.\n\n### Schema\n\nA table has a schema that Ibis maps to the data platform's data types:\n\n```{python}\nt.schema()\n```\n\n## LLMs and data: Marvin and Ibis\n\nYou can use Marvin and Ibis together to easily apply LLMs to data.\n\n```{python}\nfrom ibis.expr.schema import Schema\nfrom ibis.expr.types.relations import Table\n\n@marvin.ai_fn\ndef sql_select(\n    text: str, table_name: str = t.get_name(), schema: Schema = t.schema()\n) -> str:\n    \"\"\"writes the SQL SELECT statement to query the table according to the text\"\"\"\n\n\nquery = \"the unique combination of species and islands\"\nsql = sql_select(query).strip(\";\")\nsql\n```\n\n```{python}\nt.sql(sql)\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt.sql(sql_select(query + \" and include their counts in from highest to lowest\").strip(\";\"))\n```\n\n## Next steps\n\nYou can get involved with [Ibis\nBirdbrain](https://github.com/ibis-project/ibis-birdbrain), our open-source data\n& AI project for building next-generation natural language interfaces to data.\n\n[Read the next post in this series](../llms-and-data-pt1).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.550","resources":["CNAME"],"theme":{"dark":"darkly","light":"darkly"},"title-block-banner":true,"code-annotations":"hover","title":"An introduction to Marvin and Ibis","author":"Cody Peterson","date":"2023-10-12","categories":["LLMs and data"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}