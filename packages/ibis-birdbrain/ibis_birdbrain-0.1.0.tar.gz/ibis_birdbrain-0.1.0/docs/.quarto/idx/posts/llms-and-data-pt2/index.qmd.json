{"title":"Computations and control flow: it's just programming","markdown":{"yaml":{"title":"Computations and control flow: it's just programming","author":"Cody Peterson","date":"2023-10-14","categories":["LLMs and data"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThe recent Generative AI hype cycle has led to a lot of new terminology to\nunderstand. In this post, we'll cover some key concepts from the groud up and\nexplain the basics of working with LLMs in the context of data.\n\nThis post assumes [basic familiarity with Marvin and Ibis](../llms-and-data-pt0)\nand [three approaches to applying LLMs to data](../llms-and-data-pt1).\n\n```{python}\n# | code-fold: true\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom dotenv import load_dotenv  # <1>\n\nload_dotenv()  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = ibis.examples.penguins.fetch()  # <3>\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)  # <3>\n```\n\n1. Import the libraries we need.\n2. Load the environment variable to setup Marvin to call our OpenAI account.\n3. Setup the demo datain an Ibis backend.\n\nFirst, we'll setup Ibis and Marvin with some simple example data:\n\n```{python}\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom ibis.expr.schema import Schema  # <1>\nfrom ibis.expr.types.relations import Table  # <1>\n\nibis.options.interactive = True  # <2>\nmarvin.settings.llm_model = \"openai/gpt-4\"  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = con.table(\"penguins\")  # <3>\n```\n\n1. Import Ibis and Marvin.\n2. Configure Ibis (interactive) and Marvin (GPT-4).\n3. Connect to the data and load a table into a variable.\n\n## Context\n\nContext is a fancy way of talking about the input to a LLM.\n\n## Calls\n\nWe make calls with inputs to functions or systems and get outputs. We can think\nof calling the LLM with our input (context) and getting an output (text).\n\n## Computations\n\nA function or system often computes something. We can be pedantic about calls\nversus computations, but in general the connotation around computations is more\ntime and resource intensive than a call. At the end of the day, they will both\ntake some computer cycles.\n\n## Retrieval augmented generation (RAG)\n\nInstead of you typing out context for the bot, we can **retrieve** context from\nsomewhere, **augment** our strings sent to the bot with this context, and then\n**generate** a response from the bot.\n\nAs a contrived example, instead of saying \"The capitol of foo is bar\", we can\nretrieve the capitol of foo from a database, augment it with our context, and\nthen generate a response from the bot. You may notice that [we already did this\nin the firt post in the series -- let's review that code\nagain](../llms-and-data-pt0):\n\n```{python}\nfrom ibis.expr.schema import Schema\nfrom ibis.expr.types.relations import Table\n\n\n@marvin.ai_fn\ndef sql_select(\n    text: str, table_name: str = t.get_name(), schema: Schema = t.schema()\n) -> str:\n    \"\"\"writes the SQL SELECT statement to query the table according to the text\"\"\"\n\n\nquery = \"the unique combination of species and islands\"\nsql = sql_select(query).strip(\";\")\nsql\n```\n\nNotice that we **retrieved** the table name and schema with calls to the Ibis\ntable (`t.get_name()` and `t.schema()`). We then **augment** our context (the\nquery in natural language) with this information and **generate** a response\nfrom the bot.\n\nThis works reasonably well for simple SQL queries:\n\n```{python}\nt.sql(sql)\n```\n\nI would argue in this case there wasn't any real **computation** done by our\n**calls** to the Ibis table -- we were just retrieving some relatively static\nmetadata -- but we could have done some more complex computations (on any of 18+\ndata platforms).\n\n## Thought leadership\n\nTODO: human rewrite\n\nIn the realm of Generative AI, particularly when working with Language Learning\nModels (LLMs), understanding the concept of 'context' is crucial. Context, in this\ndomain, refers to the inputs that are fed into an LLM, and the corresponding\noutputs they generate. This post breaks down the complexities of this process into\nunderstandable fragments, including retrieval of context, its augmentation, and,\nthereafter, the generation of a response.\n\nAn illustrative example is provided, showcasing a database interaction. It\ndemonstrates how the data retrieved can be used to augment the context before the\nbot generates a response. This valuable insight underlines the practical\napplication of the theory, reinforcing the understanding of the readers.\n\nWe also venture into the difference between simple static metadata retrieval and\nthe more intricate computations. This distinction echoes the breadth and depth of\nthe processes involved in Generative AI.\n\nAs we continue to explore and unravel the potential of Generative AI and LLMs,\nthis post serves as a fundamental building block. It creates a pathway for\nenthusiasts and professionals alike to delve deeper into this exciting field. By\nbreaking down complex concepts into comprehensible segments, it fosters an\nenvironment of learning and growth.\n\nThis marks just the beginning of our journey into the world of Generative AI. As\nwe dig deeper, we will continue to explore, learn and share with our readers. Stay\ntuned for more insightful content in this series. [1]\n\n[1] https://github.com/ibis-project/ibis-birdbrain\n\n## Next steps\n\nYou can get involved with [Ibis\nBirdbrain](https://github.com/ibis-project/ibis-birdbrain), our open-source data\n& AI project for building next-generation natural language interfaces to data.\n\n[Read the next post in this series](../llms-and-data-pt3).\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nThe recent Generative AI hype cycle has led to a lot of new terminology to\nunderstand. In this post, we'll cover some key concepts from the groud up and\nexplain the basics of working with LLMs in the context of data.\n\nThis post assumes [basic familiarity with Marvin and Ibis](../llms-and-data-pt0)\nand [three approaches to applying LLMs to data](../llms-and-data-pt1).\n\n```{python}\n# | code-fold: true\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom dotenv import load_dotenv  # <1>\n\nload_dotenv()  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = ibis.examples.penguins.fetch()  # <3>\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)  # <3>\n```\n\n1. Import the libraries we need.\n2. Load the environment variable to setup Marvin to call our OpenAI account.\n3. Setup the demo datain an Ibis backend.\n\nFirst, we'll setup Ibis and Marvin with some simple example data:\n\n```{python}\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom ibis.expr.schema import Schema  # <1>\nfrom ibis.expr.types.relations import Table  # <1>\n\nibis.options.interactive = True  # <2>\nmarvin.settings.llm_model = \"openai/gpt-4\"  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = con.table(\"penguins\")  # <3>\n```\n\n1. Import Ibis and Marvin.\n2. Configure Ibis (interactive) and Marvin (GPT-4).\n3. Connect to the data and load a table into a variable.\n\n## Context\n\nContext is a fancy way of talking about the input to a LLM.\n\n## Calls\n\nWe make calls with inputs to functions or systems and get outputs. We can think\nof calling the LLM with our input (context) and getting an output (text).\n\n## Computations\n\nA function or system often computes something. We can be pedantic about calls\nversus computations, but in general the connotation around computations is more\ntime and resource intensive than a call. At the end of the day, they will both\ntake some computer cycles.\n\n## Retrieval augmented generation (RAG)\n\nInstead of you typing out context for the bot, we can **retrieve** context from\nsomewhere, **augment** our strings sent to the bot with this context, and then\n**generate** a response from the bot.\n\nAs a contrived example, instead of saying \"The capitol of foo is bar\", we can\nretrieve the capitol of foo from a database, augment it with our context, and\nthen generate a response from the bot. You may notice that [we already did this\nin the firt post in the series -- let's review that code\nagain](../llms-and-data-pt0):\n\n```{python}\nfrom ibis.expr.schema import Schema\nfrom ibis.expr.types.relations import Table\n\n\n@marvin.ai_fn\ndef sql_select(\n    text: str, table_name: str = t.get_name(), schema: Schema = t.schema()\n) -> str:\n    \"\"\"writes the SQL SELECT statement to query the table according to the text\"\"\"\n\n\nquery = \"the unique combination of species and islands\"\nsql = sql_select(query).strip(\";\")\nsql\n```\n\nNotice that we **retrieved** the table name and schema with calls to the Ibis\ntable (`t.get_name()` and `t.schema()`). We then **augment** our context (the\nquery in natural language) with this information and **generate** a response\nfrom the bot.\n\nThis works reasonably well for simple SQL queries:\n\n```{python}\nt.sql(sql)\n```\n\nI would argue in this case there wasn't any real **computation** done by our\n**calls** to the Ibis table -- we were just retrieving some relatively static\nmetadata -- but we could have done some more complex computations (on any of 18+\ndata platforms).\n\n## Thought leadership\n\nTODO: human rewrite\n\nIn the realm of Generative AI, particularly when working with Language Learning\nModels (LLMs), understanding the concept of 'context' is crucial. Context, in this\ndomain, refers to the inputs that are fed into an LLM, and the corresponding\noutputs they generate. This post breaks down the complexities of this process into\nunderstandable fragments, including retrieval of context, its augmentation, and,\nthereafter, the generation of a response.\n\nAn illustrative example is provided, showcasing a database interaction. It\ndemonstrates how the data retrieved can be used to augment the context before the\nbot generates a response. This valuable insight underlines the practical\napplication of the theory, reinforcing the understanding of the readers.\n\nWe also venture into the difference between simple static metadata retrieval and\nthe more intricate computations. This distinction echoes the breadth and depth of\nthe processes involved in Generative AI.\n\nAs we continue to explore and unravel the potential of Generative AI and LLMs,\nthis post serves as a fundamental building block. It creates a pathway for\nenthusiasts and professionals alike to delve deeper into this exciting field. By\nbreaking down complex concepts into comprehensible segments, it fosters an\nenvironment of learning and growth.\n\nThis marks just the beginning of our journey into the world of Generative AI. As\nwe dig deeper, we will continue to explore, learn and share with our readers. Stay\ntuned for more insightful content in this series. [1]\n\n[1] https://github.com/ibis-project/ibis-birdbrain\n\n## Next steps\n\nYou can get involved with [Ibis\nBirdbrain](https://github.com/ibis-project/ibis-birdbrain), our open-source data\n& AI project for building next-generation natural language interfaces to data.\n\n[Read the next post in this series](../llms-and-data-pt3).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.550","resources":["CNAME"],"theme":{"dark":"darkly","light":"darkly"},"title-block-banner":true,"code-annotations":"hover","title":"Computations and control flow: it's just programming","author":"Cody Peterson","date":"2023-10-14","categories":["LLMs and data"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}