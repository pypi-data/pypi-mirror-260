{"title":"Three approaches","markdown":{"yaml":{"title":"Three approaches","author":"Cody Peterson","date":"2023-10-13","execute":{"warning":false},"categories":["LLMs and data"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThe thought of using natural language to transform and analyze data is\nappealing. This post assumes familiarity with Marvin and Ibis -- [read the\nprevious post in the series for a quick overview](../llms-and-data-pt0).\n\n## Approaches\n\nWhen discussed at Voltron Data, we identified three distinct approaches to\napplying LLMs to data analytics that can be implemented today:\n\n1. LLM writes an analytic code\n2. LLM writes an analytic subroutine\n3. Use LLM in an analytic subroutine\n\nWhile these three approaches are not an exhaustive list of how LLMs can be\napplied to data, they can be easily understood and implemented with Ibis and\nMarvin in a few lines of code. Together with these two open-source tools, we can\nbuild a natural language interface for data analytics that supports 18+\nbackends.\n\nBut first, let's demonstrate the three approaches.\n\n### Approach 1: LLM writes analytic code\n\nState of the art (SoTA) LLMs are decent at generating SQL out of the box. We can\nbe clever to handle errors, retries, and more, but in its simplest form:\n\n```{python}\n# | code-fold: true\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom rich import print # <1>\nfrom time import sleep  # <1>\nfrom dotenv import load_dotenv  # <1>\n\nload_dotenv()  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = ibis.examples.penguins.fetch()  # <3>\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)  # <3>\n```\n\n1. Import the libraries we need.\n2. Load the environment variable to setup Marvin to call our OpenAI account.\n3. Setup the demo datain an Ibis backend.\n\n```{python}\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom ibis.expr.schema import Schema  # <1>\nfrom ibis.expr.types.relations import Table  # <1>\n\n\nibis.options.interactive = True # <2>\nmarvin.settings.llm_model = \"openai/gpt-4\"  # <2>\n```\n\n1. Import Ibis and Marvin.\n2. Configure Ibis and Marvin\n\n```{python}\n@marvin.ai_fn  # <1>\ndef _generate_sql_select(\n    text: str, table_name: str, table_schema: Schema\n) -> str:  # <1>\n    \"\"\"Generate SQL SELECT from text.\"\"\"  # <1>\n\n\ndef sql_from_text(text: str, t: Table) -> Table:  # <2>\n    \"\"\"Run SQL from text.\"\"\"  # <2>\n    return t.sql(_generate_sql_select(text, t.get_name(), t.schema()).strip(\";\"))  # <2>\n```\n\n1. A non-deterministic, LLM-powered AI function.\n2. A deterministic, human-authored function that calls the AI function.\n\n```{python}\nt2 = sql_from_text(\"the unique combination of species and islands\", t)\nt2\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt3 = sql_from_text(\n    \"the unique combination of species and islands, with their counts, ordered from highest to lowest, and name that column just 'count'\",\n    t,\n)\nt3\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\nThis works well-enough for simple cases and can be expanded to handle complex\nones. In many scenarios, it may be easier to express a query in English or\nanother language than to write it in SQL, especially if working across multiple\nSQL dialects.\n\nSQL isn't standard, with many dialects across data platforms. Ibis works around\nthis by providing a standard Python API for analytic code but must make\ncompromises to support many data platforms, often via SQL in their native\ndialect. [Substrait](https://substrait.io) is a newer project that aims to solve\nthis problem by providing a standard, portable, and extensible intermediary\nrepresentation (IR) for data transformation code that Ibis and data platforms\ncould all standardize on. Substrait is still in the early stages of development,\nbut it's worth keeping an eye on and will be adopted in Ibis once supported\nacross many data platforms.\n\nFor now, we'll focus on generating SQL and Python analytical code with LLMs.\n\n### Approach 2: LLM writes an analytical subroutine\n\nIf more complex logic needs to be expressed, SoTA LLMs are also decent at\nwriting Python and a number of other programming languages that are used in\nanalytical subroutines. Many data platforms support user-defined functions\n(UDFs) in Python or some other language. We'll stick to scalar Python UDFs via\nDuckDB to demonstrate the concept:\n\n```{python}\n@marvin.ai_fn  # <1>\ndef _generate_python_function(text: str) -> str:  # <1>\n    \"\"\"Generate a simple, typed, correct Python function from text.\"\"\"  # <1>\n\n\ndef create_udf_from_text(text: str) -> str:  # <2>\n    \"\"\"Create a UDF from text.\"\"\"  # <2>\n    return f\"\"\"\nimport ibis\n\n@ibis.udf.scalar.python\n{_generate_python_function(text)}\n\"\"\".strip()  # <2>\n```\n\n1. A non-deterministic, LLM-powered AI function.\n2. A deterministic, human-authored function that calls the AI function.\n\n```{python}\nudf = create_udf_from_text(\n    \"a function named count_vowels that given an input string, returns an int w/ the number of vowels (y_included as a boolean option defaulted to False)\"\n)\nprint(udf)\nexec(udf)\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt4 = t3.mutate(\n    species_vowel_count=count_vowels(t3.species),\n    island_vowel_count=count_vowels(t3.island),\n)\nt4\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\nIn this case, there's no reason not to have a human in the loop reviewing the\noutput code and committing it for production use. This could be useful for quick\nprototyping or, given a box of tools in the form of UDFs,\nworking through a natural language interface.\n\n### Approach 3: Use LLM in an analytical subroutine\n\nWe can also call the LLM once-per-row in the table via a subroutine. For\nvariety, we'll use an [AI model](https://www.askmarvin.ai/components/ai_model/)\ninstead of an [AI function](https://www.askmarvin.ai/components/ai_function/):\n\n```{python}\nfrom pydantic import BaseModel, Field  # <1>\n\n# decrease cost\nmarvin.settings.llm_model = \"openai/gpt-3.5-turbo-16k\"  # <2>\n\n\n@marvin.ai_model  # <3>\nclass VowelCounter(BaseModel):  # <3>\n    \"\"\"Count vowels in a string.\"\"\"  # <3>\n\n    include_y: bool = Field(False, description=\"Include 'y' as a vowel.\")  # <3>\n    # num_a: int = Field(..., description=\"The number of 'a' vowels.\") # <3>\n    # num_e: int = Field(..., description=\"The number of 'e' vowels.\") # <3>\n    # num_i: int = Field(..., description=\"The number of 'i' vowels.\") # <3>\n    # num_o: int = Field(..., description=\"The number of 'o' vowels.\") # <3>\n    # num_u: int = Field(..., description=\"The number of 'u' vowels.\") # <3>\n    # num_y: int = Field(..., description=\"The number of 'y' vowels.\") # <3>\n    num_total: int = Field(..., description=\"The total number of vowels.\")  # <3>\n\n\nVowelCounter(\"hello world\")  # <4>\n```\n\n1. Additional imports for Pydantic.\n2. Configure Marvin to use a cheaper model.\n3. A non-deterministic, LLM-powered AI model.\n4. Call the AI model on some text.\n\nThen we'll have the LLM write the UDF that calls the LLM, just to be fancy:\n\n```{python}\nudf = create_udf_from_text(\n    \"a function named count_vowels_ai that given an input string, calls VowelCounter on it and returns the num_total attribute of that result\"\n)\nprint(udf)\nexec(udf)\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt5 = t3.mutate(\n    species_vowel_count=count_vowels_ai(t3.species),\n    island_vowel_count=count_vowels_ai(t3.island),\n)\nt5\n```\n\nNotice that in this UDF, unlike in the previous example, a LLM is being called\n(possibly several times) for each row in the table. This is a very expensive\noperation and we'll need to be careful about how we use it in practice.\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n## Summary\n\nTo summarize this post:\n\n```{python}\nfrom rich import print\n\nwith open(\"index.qmd\", \"r\") as f:\n    self_text = f.read()\n\n# increase accuracy\nmarvin.settings.llm_model = \"openai/gpt-4\"\n\n@marvin.ai_model\nclass Summary(BaseModel):\n    \"\"\"Summary of text.\"\"\"\n\n    summary_line: str = Field(..., description=\"The one-line summary of the text.\")\n    summary_paragraph: str = Field(\n        ..., description=\"The one-paragraph summary of the text.\"\n    )\n    conclusion: str = Field(\n        ..., description=\"The conclusion the reader should draw from the text.\"\n    )\n    key_points: list[str] = Field(..., description=\"The key points of the text.\")\n    critiques: list[str] = Field(\n        ..., description=\"Professional, fair critiques of the text.\"\n    )\n    suggested_improvements: list[str] = Field(\n        ..., description=\"Suggested improvements for the text.\"\n    )\n    sentiment: float = Field(..., description=\"The sentiment of the text.\")\n    sentiment_label: str = Field(..., description=\"The sentiment label of the text.\")\n    author_bias: str = Field(..., description=\"The author bias of the text.\")\n\n\nprint(Summary(self_text))\n```\n\n## Next steps\n\nYou can get involved with [Ibis\nBirdbrain](https://github.com/ibis-project/ibis-birdbrain), our open-source data\n& AI project for building next-generation natural language interfaces to data.\n\n[Read the next post in this series](../llms-and-data-pt2).\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nThe thought of using natural language to transform and analyze data is\nappealing. This post assumes familiarity with Marvin and Ibis -- [read the\nprevious post in the series for a quick overview](../llms-and-data-pt0).\n\n## Approaches\n\nWhen discussed at Voltron Data, we identified three distinct approaches to\napplying LLMs to data analytics that can be implemented today:\n\n1. LLM writes an analytic code\n2. LLM writes an analytic subroutine\n3. Use LLM in an analytic subroutine\n\nWhile these three approaches are not an exhaustive list of how LLMs can be\napplied to data, they can be easily understood and implemented with Ibis and\nMarvin in a few lines of code. Together with these two open-source tools, we can\nbuild a natural language interface for data analytics that supports 18+\nbackends.\n\nBut first, let's demonstrate the three approaches.\n\n### Approach 1: LLM writes analytic code\n\nState of the art (SoTA) LLMs are decent at generating SQL out of the box. We can\nbe clever to handle errors, retries, and more, but in its simplest form:\n\n```{python}\n# | code-fold: true\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom rich import print # <1>\nfrom time import sleep  # <1>\nfrom dotenv import load_dotenv  # <1>\n\nload_dotenv()  # <2>\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")  # <3>\nt = ibis.examples.penguins.fetch()  # <3>\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)  # <3>\n```\n\n1. Import the libraries we need.\n2. Load the environment variable to setup Marvin to call our OpenAI account.\n3. Setup the demo datain an Ibis backend.\n\n```{python}\nimport ibis  # <1>\nimport marvin  # <1>\n\nfrom ibis.expr.schema import Schema  # <1>\nfrom ibis.expr.types.relations import Table  # <1>\n\n\nibis.options.interactive = True # <2>\nmarvin.settings.llm_model = \"openai/gpt-4\"  # <2>\n```\n\n1. Import Ibis and Marvin.\n2. Configure Ibis and Marvin\n\n```{python}\n@marvin.ai_fn  # <1>\ndef _generate_sql_select(\n    text: str, table_name: str, table_schema: Schema\n) -> str:  # <1>\n    \"\"\"Generate SQL SELECT from text.\"\"\"  # <1>\n\n\ndef sql_from_text(text: str, t: Table) -> Table:  # <2>\n    \"\"\"Run SQL from text.\"\"\"  # <2>\n    return t.sql(_generate_sql_select(text, t.get_name(), t.schema()).strip(\";\"))  # <2>\n```\n\n1. A non-deterministic, LLM-powered AI function.\n2. A deterministic, human-authored function that calls the AI function.\n\n```{python}\nt2 = sql_from_text(\"the unique combination of species and islands\", t)\nt2\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt3 = sql_from_text(\n    \"the unique combination of species and islands, with their counts, ordered from highest to lowest, and name that column just 'count'\",\n    t,\n)\nt3\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\nThis works well-enough for simple cases and can be expanded to handle complex\nones. In many scenarios, it may be easier to express a query in English or\nanother language than to write it in SQL, especially if working across multiple\nSQL dialects.\n\nSQL isn't standard, with many dialects across data platforms. Ibis works around\nthis by providing a standard Python API for analytic code but must make\ncompromises to support many data platforms, often via SQL in their native\ndialect. [Substrait](https://substrait.io) is a newer project that aims to solve\nthis problem by providing a standard, portable, and extensible intermediary\nrepresentation (IR) for data transformation code that Ibis and data platforms\ncould all standardize on. Substrait is still in the early stages of development,\nbut it's worth keeping an eye on and will be adopted in Ibis once supported\nacross many data platforms.\n\nFor now, we'll focus on generating SQL and Python analytical code with LLMs.\n\n### Approach 2: LLM writes an analytical subroutine\n\nIf more complex logic needs to be expressed, SoTA LLMs are also decent at\nwriting Python and a number of other programming languages that are used in\nanalytical subroutines. Many data platforms support user-defined functions\n(UDFs) in Python or some other language. We'll stick to scalar Python UDFs via\nDuckDB to demonstrate the concept:\n\n```{python}\n@marvin.ai_fn  # <1>\ndef _generate_python_function(text: str) -> str:  # <1>\n    \"\"\"Generate a simple, typed, correct Python function from text.\"\"\"  # <1>\n\n\ndef create_udf_from_text(text: str) -> str:  # <2>\n    \"\"\"Create a UDF from text.\"\"\"  # <2>\n    return f\"\"\"\nimport ibis\n\n@ibis.udf.scalar.python\n{_generate_python_function(text)}\n\"\"\".strip()  # <2>\n```\n\n1. A non-deterministic, LLM-powered AI function.\n2. A deterministic, human-authored function that calls the AI function.\n\n```{python}\nudf = create_udf_from_text(\n    \"a function named count_vowels that given an input string, returns an int w/ the number of vowels (y_included as a boolean option defaulted to False)\"\n)\nprint(udf)\nexec(udf)\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt4 = t3.mutate(\n    species_vowel_count=count_vowels(t3.species),\n    island_vowel_count=count_vowels(t3.island),\n)\nt4\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\nIn this case, there's no reason not to have a human in the loop reviewing the\noutput code and committing it for production use. This could be useful for quick\nprototyping or, given a box of tools in the form of UDFs,\nworking through a natural language interface.\n\n### Approach 3: Use LLM in an analytical subroutine\n\nWe can also call the LLM once-per-row in the table via a subroutine. For\nvariety, we'll use an [AI model](https://www.askmarvin.ai/components/ai_model/)\ninstead of an [AI function](https://www.askmarvin.ai/components/ai_function/):\n\n```{python}\nfrom pydantic import BaseModel, Field  # <1>\n\n# decrease cost\nmarvin.settings.llm_model = \"openai/gpt-3.5-turbo-16k\"  # <2>\n\n\n@marvin.ai_model  # <3>\nclass VowelCounter(BaseModel):  # <3>\n    \"\"\"Count vowels in a string.\"\"\"  # <3>\n\n    include_y: bool = Field(False, description=\"Include 'y' as a vowel.\")  # <3>\n    # num_a: int = Field(..., description=\"The number of 'a' vowels.\") # <3>\n    # num_e: int = Field(..., description=\"The number of 'e' vowels.\") # <3>\n    # num_i: int = Field(..., description=\"The number of 'i' vowels.\") # <3>\n    # num_o: int = Field(..., description=\"The number of 'o' vowels.\") # <3>\n    # num_u: int = Field(..., description=\"The number of 'u' vowels.\") # <3>\n    # num_y: int = Field(..., description=\"The number of 'y' vowels.\") # <3>\n    num_total: int = Field(..., description=\"The total number of vowels.\")  # <3>\n\n\nVowelCounter(\"hello world\")  # <4>\n```\n\n1. Additional imports for Pydantic.\n2. Configure Marvin to use a cheaper model.\n3. A non-deterministic, LLM-powered AI model.\n4. Call the AI model on some text.\n\nThen we'll have the LLM write the UDF that calls the LLM, just to be fancy:\n\n```{python}\nudf = create_udf_from_text(\n    \"a function named count_vowels_ai that given an input string, calls VowelCounter on it and returns the num_total attribute of that result\"\n)\nprint(udf)\nexec(udf)\n```\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n```{python}\nt5 = t3.mutate(\n    species_vowel_count=count_vowels_ai(t3.species),\n    island_vowel_count=count_vowels_ai(t3.island),\n)\nt5\n```\n\nNotice that in this UDF, unlike in the previous example, a LLM is being called\n(possibly several times) for each row in the table. This is a very expensive\noperation and we'll need to be careful about how we use it in practice.\n\n```{python}\n# | code-fold: true\nsleep(3) # <1>\n```\n\n1. Avoid rate-limiting by waiting.\n\n## Summary\n\nTo summarize this post:\n\n```{python}\nfrom rich import print\n\nwith open(\"index.qmd\", \"r\") as f:\n    self_text = f.read()\n\n# increase accuracy\nmarvin.settings.llm_model = \"openai/gpt-4\"\n\n@marvin.ai_model\nclass Summary(BaseModel):\n    \"\"\"Summary of text.\"\"\"\n\n    summary_line: str = Field(..., description=\"The one-line summary of the text.\")\n    summary_paragraph: str = Field(\n        ..., description=\"The one-paragraph summary of the text.\"\n    )\n    conclusion: str = Field(\n        ..., description=\"The conclusion the reader should draw from the text.\"\n    )\n    key_points: list[str] = Field(..., description=\"The key points of the text.\")\n    critiques: list[str] = Field(\n        ..., description=\"Professional, fair critiques of the text.\"\n    )\n    suggested_improvements: list[str] = Field(\n        ..., description=\"Suggested improvements for the text.\"\n    )\n    sentiment: float = Field(..., description=\"The sentiment of the text.\")\n    sentiment_label: str = Field(..., description=\"The sentiment label of the text.\")\n    author_bias: str = Field(..., description=\"The author bias of the text.\")\n\n\nprint(Summary(self_text))\n```\n\n## Next steps\n\nYou can get involved with [Ibis\nBirdbrain](https://github.com/ibis-project/ibis-birdbrain), our open-source data\n& AI project for building next-generation natural language interfaces to data.\n\n[Read the next post in this series](../llms-and-data-pt2).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.550","resources":["CNAME"],"theme":{"dark":"darkly","light":"darkly"},"title-block-banner":true,"code-annotations":"hover","title":"Three approaches","author":"Cody Peterson","date":"2023-10-13","categories":["LLMs and data"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}